{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram using Navie-Softmax- CS224n\n",
    "import the neccesarry libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the version of torch and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use suitable device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "#torch.cuda.set_device(gpus[0])\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function \"getBatch\" accourding to batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare sequences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download gutenberg text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/bharat/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the corpus .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download punkt package used for tokens creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bharat/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one .txt file and print list of corpus upto 100 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] # sampling sentences for test\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'], ['etymology', '.'], ['(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')'], ['the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.'], ['he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.'], ['he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.'], ['\"', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-', 'fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'h', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.\"'], ['--', 'hackluyt'], ['\"', 'whale', '.'], ['...', 'sw', '.', 'and', 'dan', '.'], ['hval', '.'], ['this', 'animal', 'is', 'named', 'from', 'roundness', 'or', 'rolling', ';', 'for', 'in', 'dan', '.'], ['hvalt', 'is', 'arched', 'or', 'vaulted', '.\"'], ['--', 'webster', \"'\", 's', 'dictionary'], ['\"', 'whale', '.'], ['...'], ['it', 'is', 'more', 'immediately', 'from', 'the', 'dut', '.'], ['and', 'ger', '.'], ['wallen', ';', 'a', '.', 's', '.', 'walw', '-', 'ian', ',', 'to', 'roll', ',', 'to', 'wallow', '.\"'], ['--', 'richardson', \"'\", 's', 'dictionary'], ['ketos', ',', 'greek', '.'], ['cetus', ',', 'latin', '.'], ['whoel', ',', 'anglo', '-', 'saxon', '.'], ['hvalt', ',', 'danish', '.'], ['wal', ',', 'dutch', '.'], ['hwal', ',', 'swedish', '.'], ['whale', ',', 'icelandic', '.'], ['whale', ',', 'english', '.'], ['baleine', ',', 'french', '.'], ['ballena', ',', 'spanish', '.'], ['pekee', '-', 'nuee', '-', 'nuee', ',', 'fegee', '.'], ['pekee', '-', 'nuee', '-', 'nuee', ',', 'erromangoan', '.'], ['extracts', '(', 'supplied', 'by', 'a', 'sub', '-', 'sub', '-', 'librarian', ').'], ['it', 'will', 'be', 'seen', 'that', 'this', 'mere', 'painstaking', 'burrower', 'and', 'grub', '-', 'worm', 'of', 'a', 'poor', 'devil', 'of', 'a', 'sub', '-', 'sub', 'appears', 'to', 'have', 'gone', 'through', 'the', 'long', 'vaticans', 'and', 'street', '-', 'stalls', 'of', 'the', 'earth', ',', 'picking', 'up', 'whatever', 'random', 'allusions', 'to', 'whales', 'he', 'could', 'anyways', 'find', 'in', 'any', 'book', 'whatsoever', ',', 'sacred', 'or', 'profane', '.'], ['therefore', 'you', 'must', 'not', ',', 'in', 'every', 'case', 'at', 'least', ',', 'take', 'the', 'higgledy', '-', 'piggledy', 'whale', 'statements', ',', 'however', 'authentic', ',', 'in', 'these', 'extracts', ',', 'for', 'veritable', 'gospel', 'cetology', '.'], ['far', 'from', 'it', '.'], ['as', 'touching', 'the', 'ancient', 'authors', 'generally', ',', 'as', 'well', 'as', 'the', 'poets', 'here', 'appearing', ',', 'these', 'extracts', 'are', 'solely', 'valuable', 'or', 'entertaining', ',', 'as', 'affording', 'a', 'glancing', 'bird', \"'\", 's', 'eye', 'view', 'of', 'what', 'has', 'been', 'promiscuously', 'said', ',', 'thought', ',', 'fancied', ',', 'and', 'sung', 'of', 'leviathan', ',', 'by', 'many', 'nations', 'and', 'generations', ',', 'including', 'our', 'own', '.'], ['so', 'fare', 'thee', 'well', ',', 'poor', 'devil', 'of', 'a', 'sub', '-', 'sub', ',', 'whose', 'commentator', 'i', 'am', '.'], ['thou', 'belongest', 'to', 'that', 'hopeless', ',', 'sallow', 'tribe', 'which', 'no', 'wine', 'of', 'this', 'world', 'will', 'ever', 'warm', ';', 'and', 'for', 'whom', 'even', 'pale', 'sherry', 'would', 'be', 'too', 'rosy', '-', 'strong', ';', 'but', 'with', 'whom', 'one', 'sometimes', 'loves', 'to', 'sit', ',', 'and', 'feel', 'poor', '-', 'devilish', ',', 'too', ';', 'and', 'grow', 'convivial', 'upon', 'tears', ';', 'and', 'say', 'to', 'them', 'bluntly', ',', 'with', 'full', 'eyes', 'and', 'empty', 'glasses', ',', 'and', 'in', 'not', 'altogether', 'unpleasant', 'sadness', '--', 'give', 'it', 'up', ',', 'sub', '-', 'subs', '!'], ['for', 'by', 'how', 'much', 'the', 'more', 'pains', 'ye', 'take', 'to', 'please', 'the', 'world', ',', 'by', 'so', 'much', 'the', 'more', 'shall', 'ye', 'for', 'ever', 'go', 'thankless', '!'], ['would', 'that', 'i', 'could', 'clear', 'out', 'hampton', 'court', 'and', 'the', 'tuileries', 'for', 'ye', '!'], ['but', 'gulp', 'down', 'your', 'tears', 'and', 'hie', 'aloft', 'to', 'the', 'royal', '-', 'mast', 'with', 'your', 'hearts', ';', 'for', 'your', 'friends', 'who', 'have', 'gone', 'before', 'are', 'clearing', 'out', 'the', 'seven', '-', 'storied', 'heavens', ',', 'and', 'making', 'refugees', 'of', 'long', '-', 'pampered', 'gabriel', ',', 'michael', ',', 'and', 'raphael', ',', 'against', 'your', 'coming', '.'], ['here', 'ye', 'strike', 'but', 'splintered', 'hearts', 'together', '--', 'there', ',', 'ye', 'shall', 'strike', 'unsplinterable', 'glasses', '!'], ['extracts', '.'], ['\"', 'and', 'god', 'created', 'great', 'whales', '.\"'], ['--', 'genesis', '.'], ['\"', 'leviathan', 'maketh', 'a', 'path', 'to', 'shine', 'after', 'him', ';', 'one', 'would', 'think', 'the', 'deep', 'to', 'be', 'hoary', '.\"'], ['--', 'job', '.'], ['\"', 'now', 'the', 'lord', 'had', 'prepared', 'a', 'great', 'fish', 'to', 'swallow', 'up', 'jonah', '.\"'], ['--', 'jonah', '.'], ['\"', 'there', 'go', 'the', 'ships', ';', 'there', 'is', 'that', 'leviathan', 'whom', 'thou', 'hast', 'made', 'to', 'play', 'therein', '.\"'], ['--', 'psalms', '.'], ['\"', 'in', 'that', 'day', ',', 'the', 'lord', 'with', 'his', 'sore', ',', 'and', 'great', ',', 'and', 'strong', 'sword', ',', 'shall', 'punish', 'leviathan', 'the', 'piercing', 'serpent', ',', 'even', 'leviathan', 'that', 'crooked', 'serpent', ';', 'and', 'he', 'shall', 'slay', 'the', 'dragon', 'that', 'is', 'in', 'the', 'sea', '.\"'], ['--', 'isaiah'], ['\"', 'and', 'what', 'thing', 'soever', 'besides', 'cometh', 'within', 'the', 'chaos', 'of', 'this', 'monster', \"'\", 's', 'mouth', ',', 'be', 'it', 'beast', ',', 'boat', ',', 'or', 'stone', ',', 'down', 'it', 'goes', 'all', 'incontinently', 'that', 'foul', 'great', 'swallow', 'of', 'his', ',', 'and', 'perisheth', 'in', 'the', 'bottomless', 'gulf', 'of', 'his', 'paunch', '.\"'], ['--', 'holland', \"'\", 's', 'plutarch', \"'\", 's', 'morals', '.'], ['\"', 'the', 'indian', 'sea', 'breedeth', 'the', 'most', 'and', 'the', 'biggest', 'fishes', 'that', 'are', ':', 'among', 'which', 'the', 'whales', 'and', 'whirlpooles', 'called', 'balaene', ',', 'take', 'up', 'as', 'much', 'in', 'length', 'as', 'four', 'acres', 'or', 'arpens', 'of', 'land', '.\"'], ['--', 'holland', \"'\", 's', 'pliny', '.'], ['\"', 'scarcely', 'had', 'we', 'proceeded', 'two', 'days', 'on', 'the', 'sea', ',', 'when', 'about', 'sunrise', 'a', 'great', 'many', 'whales', 'and', 'other', 'monsters', 'of', 'the', 'sea', ',', 'appeared', '.'], ['among', 'the', 'former', ',', 'one', 'was', 'of', 'a', 'most', 'monstrous', 'size', '.'], ['...'], ['this', 'came', 'towards', 'us', ',', 'open', '-', 'mouthed', ',', 'raising', 'the', 'waves', 'on', 'all', 'sides', ',', 'and', 'beating', 'the', 'sea', 'before', 'him', 'into', 'a', 'foam', '.\"'], ['--', 'tooke', \"'\", 's', 'lucian', '.'], ['\"', 'the', 'true', 'history', '.\"'], ['\"', 'he', 'visited', 'this', 'country', 'also', 'with', 'a', 'view', 'of', 'catching', 'horse', '-', 'whales', ',', 'which', 'had', 'bones', 'of', 'very', 'great', 'value', 'for', 'their', 'teeth', ',', 'of', 'which', 'he', 'brought', 'some', 'to', 'the', 'king', '.'], ['...'], ['the', 'best', 'whales', 'were', 'catched', 'in', 'his', 'own', 'country', ',', 'of', 'which', 'some', 'were', 'forty', '-', 'eight', ',', 'some', 'fifty', 'yards', 'long', '.'], ['he', 'said', 'that', 'he', 'was', 'one', 'of', 'six', 'who', 'had', 'killed', 'sixty', 'in', 'two', 'days', '.\"'], ['--', 'other', 'or', 'octher', \"'\", 's', 'verbal', 'narrative', 'taken', 'down', 'from', 'his', 'mouth', 'by', 'king', 'alfred', ',', 'a', '.', 'd', '.', '890', '.'], ['\"', 'and', 'whereas', 'all', 'the', 'other', 'things', ',', 'whether', 'beast', 'or', 'vessel', ',', 'that', 'enter', 'into', 'the', 'dreadful', 'gulf', 'of', 'this', 'monster', \"'\", 's', '(', 'whale', \"'\", 's', ')', 'mouth', ',', 'are', 'immediately', 'lost', 'and', 'swallowed', 'up', ',', 'the', 'sea', '-', 'gudgeon', 'retires', 'into', 'it', 'in', 'great', 'security', ',', 'and', 'there', 'sleeps', '.\"'], ['--', 'montaigne', '.'], ['--', 'apology', 'for', 'raimond', 'sebond', '.'], ['\"', 'let', 'us', 'fly', ',', 'let', 'us', 'fly', '!'], ['old', 'nick', 'take', 'me', 'if', 'is', 'not', 'leviathan', 'described', 'by', 'the', 'noble', 'prophet', 'moses', 'in', 'the', 'life', 'of', 'patient', 'job', '.\"'], ['--', 'rabelais', '.'], ['\"', 'this', 'whale', \"'\", 's', 'liver', 'was', 'two', 'cartloads', '.\"'], ['--', 'stowe', \"'\", 's', 'annals', '.'], ['\"', 'the', 'great', 'leviathan', 'that', 'maketh', 'the', 'seas', 'to', 'seethe', 'like', 'boiling', 'pan', '.\"'], ['--', 'lord', 'bacon', \"'\", 's', 'version', 'of', 'the', 'psalms', '.'], ['\"', 'touching', 'that', 'monstrous', 'bulk', 'of', 'the', 'whale', 'or', 'ork', 'we', 'have', 'received', 'nothing', 'certain', '.'], ['they', 'grow', 'exceeding', 'fat', ',', 'insomuch', 'that', 'an', 'incredible', 'quantity', 'of', 'oil', 'will', 'be', 'extracted', 'out', 'of', 'one', 'whale', '.\"'], ['--', 'ibid', '.'], ['\"', 'history', 'of', 'life', 'and', 'death', '.\"'], ['\"', 'the', 'sovereignest', 'thing', 'on', 'earth', 'is', 'parmacetti', 'for', 'an', 'inward', 'bruise', '.\"'], ['--', 'king', 'henry', '.'], ['\"', 'very', 'like', 'a', 'whale', '.\"'], ['--', 'hamlet', '.'], ['\"', 'which', 'to', 'secure', ',', 'no', 'skill', 'of', 'leach', \"'\", 's', 'art', 'mote', 'him', 'availle', ',', 'but', 'to', 'returne', 'againe', 'to', 'his', 'wound', \"'\", 's', 'worker', ',', 'that', 'with', 'lowly', 'dart', ',', 'dinting', 'his', 'breast', ',', 'had', 'bred', 'his', 'restless', 'paine', ',', 'like', 'as', 'the', 'wounded', 'whale', 'to', 'shore', 'flies', 'thro', \"'\", 'the', 'maine', '.\"'], ['--', 'the', 'faerie', 'queen', '.'], ['\"', 'immense', 'as', 'whales', ',', 'the', 'motion', 'of', 'whose', 'vast', 'bodies', 'can', 'in', 'a', 'peaceful', 'calm', 'trouble', 'the', 'ocean', 'til', 'it', 'boil', '.\"'], ['--', 'sir', 'william', 'davenant', '.'], ['preface', 'to', 'gondibert', '.'], ['\"', 'what', 'spermacetti', 'is', ',', 'men', 'might', 'justly', 'doubt', ',', 'since', 'the', 'learned', 'hosmannus', 'in', 'his', 'work', 'of', 'thirty', 'years', ',', 'saith', 'plainly', ',', 'nescio', 'quid', 'sit', '.\"'], ['--', 'sir', 't', '.', 'browne', '.'], ['of', 'sperma', 'ceti', 'and', 'the', 'sperma', 'ceti', 'whale', '.'], ['vide', 'his', 'v', '.', 'e', '.'], ['\"', 'like', 'spencer', \"'\", 's', 'talus', 'with', 'his', 'modern', 'flail', 'he', 'threatens', 'ruin', 'with', 'his', 'ponderous', 'tail', '.'], ['...', 'their', 'fixed', 'jav', \"'\", 'lins', 'in', 'his', 'side', 'he', 'wears', ',', 'and', 'on', 'his', 'back', 'a', 'grove', 'of', 'pikes', 'appears', '.\"'], ['--', 'waller', \"'\", 's', 'battle', 'of', 'the', 'summer', 'islands', '.'], ['\"', 'by', 'art', 'is', 'created', 'that', 'great', 'leviathan', ',', 'called', 'a', 'commonwealth', 'or', 'state', '--(', 'in', 'latin', ',', 'civitas', ')', 'which', 'is', 'but', 'an', 'artificial', 'man', '.\"']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '\"', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-', 'fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'h', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.\"', '--', 'hackluyt', '\"', 'whale', '.', '...', 'sw', '.', 'and', 'dan', '.', 'hval', '.', 'this', 'animal', 'is', 'named', 'from', 'roundness', 'or', 'rolling', ';', 'for', 'in', 'dan', '.', 'hvalt', 'is', 'arched', 'or', 'vaulted', '.\"', '--', 'webster', \"'\", 's', 'dictionary', '\"', 'whale', '.', '...', 'it', 'is', 'more', 'immediately', 'from', 'the', 'dut', '.', 'and', 'ger', '.', 'wallen', ';', 'a', '.', 's', '.', 'walw', '-', 'ian', ',', 'to', 'roll', ',', 'to', 'wallow', '.\"', '--', 'richardson', \"'\", 's', 'dictionary', 'ketos', ',', 'greek', '.', 'cetus', ',', 'latin', '.', 'whoel', ',', 'anglo', '-', 'saxon', '.', 'hvalt', ',', 'danish', '.', 'wal', ',', 'dutch', '.', 'hwal', ',', 'swedish', '.', 'whale', ',', 'icelandic', '.', 'whale', ',', 'english', '.', 'baleine', ',', 'french', '.', 'ballena', ',', 'spanish', '.', 'pekee', '-', 'nuee', '-', 'nuee', ',', 'fegee', '.', 'pekee', '-', 'nuee', '-', 'nuee', ',', 'erromangoan', '.', 'extracts', '(', 'supplied', 'by', 'a', 'sub', '-', 'sub', '-', 'librarian', ').', 'it', 'will', 'be', 'seen', 'that', 'this', 'mere', 'painstaking', 'burrower', 'and', 'grub', '-', 'worm', 'of', 'a', 'poor', 'devil', 'of', 'a', 'sub', '-', 'sub', 'appears', 'to', 'have', 'gone', 'through', 'the', 'long', 'vaticans', 'and', 'street', '-', 'stalls', 'of', 'the', 'earth', ',', 'picking', 'up', 'whatever', 'random', 'allusions', 'to', 'whales', 'he', 'could', 'anyways', 'find', 'in', 'any', 'book', 'whatsoever', ',', 'sacred', 'or', 'profane', '.', 'therefore', 'you', 'must', 'not', ',', 'in', 'every', 'case', 'at', 'least', ',', 'take', 'the', 'higgledy', '-', 'piggledy', 'whale', 'statements', ',', 'however', 'authentic', ',', 'in', 'these', 'extracts', ',', 'for', 'veritable', 'gospel', 'cetology', '.', 'far', 'from', 'it', '.', 'as', 'touching', 'the', 'ancient', 'authors', 'generally', ',', 'as', 'well', 'as', 'the', 'poets', 'here', 'appearing', ',', 'these', 'extracts', 'are', 'solely', 'valuable', 'or', 'entertaining', ',', 'as', 'affording', 'a', 'glancing', 'bird', \"'\", 's', 'eye', 'view', 'of', 'what', 'has', 'been', 'promiscuously', 'said', ',', 'thought', ',', 'fancied', ',', 'and', 'sung', 'of', 'leviathan', ',', 'by', 'many', 'nations', 'and', 'generations', ',', 'including', 'our', 'own', '.', 'so', 'fare', 'thee', 'well', ',', 'poor', 'devil', 'of', 'a', 'sub', '-', 'sub', ',', 'whose', 'commentator', 'i', 'am', '.', 'thou', 'belongest', 'to', 'that', 'hopeless', ',', 'sallow', 'tribe', 'which', 'no', 'wine', 'of', 'this', 'world', 'will', 'ever', 'warm', ';', 'and', 'for', 'whom', 'even', 'pale', 'sherry', 'would', 'be', 'too', 'rosy', '-', 'strong', ';', 'but', 'with', 'whom', 'one', 'sometimes', 'loves', 'to', 'sit', ',', 'and', 'feel', 'poor', '-', 'devilish', ',', 'too', ';', 'and', 'grow', 'convivial', 'upon', 'tears', ';', 'and', 'say', 'to', 'them', 'bluntly', ',', 'with', 'full', 'eyes', 'and', 'empty', 'glasses', ',', 'and', 'in', 'not', 'altogether', 'unpleasant', 'sadness', '--', 'give', 'it', 'up', ',', 'sub', '-', 'subs', '!', 'for', 'by', 'how', 'much', 'the', 'more', 'pains', 'ye', 'take', 'to', 'please', 'the', 'world', ',', 'by', 'so', 'much', 'the', 'more', 'shall', 'ye', 'for', 'ever', 'go', 'thankless', '!', 'would', 'that', 'i', 'could', 'clear', 'out', 'hampton', 'court', 'and', 'the', 'tuileries', 'for', 'ye', '!', 'but', 'gulp', 'down', 'your', 'tears', 'and', 'hie', 'aloft', 'to', 'the', 'royal', '-', 'mast', 'with', 'your', 'hearts', ';', 'for', 'your', 'friends', 'who', 'have', 'gone', 'before', 'are', 'clearing', 'out', 'the', 'seven', '-', 'storied', 'heavens', ',', 'and', 'making', 'refugees', 'of', 'long', '-', 'pampered', 'gabriel', ',', 'michael', ',', 'and', 'raphael', ',', 'against', 'your', 'coming', '.', 'here', 'ye', 'strike', 'but', 'splintered', 'hearts', 'together', '--', 'there', ',', 'ye', 'shall', 'strike', 'unsplinterable', 'glasses', '!', 'extracts', '.', '\"', 'and', 'god', 'created', 'great', 'whales', '.\"', '--', 'genesis', '.', '\"', 'leviathan', 'maketh', 'a', 'path', 'to', 'shine', 'after', 'him', ';', 'one', 'would', 'think', 'the', 'deep', 'to', 'be', 'hoary', '.\"', '--', 'job', '.', '\"', 'now', 'the', 'lord', 'had', 'prepared', 'a', 'great', 'fish', 'to', 'swallow', 'up', 'jonah', '.\"', '--', 'jonah', '.', '\"', 'there', 'go', 'the', 'ships', ';', 'there', 'is', 'that', 'leviathan', 'whom', 'thou', 'hast', 'made', 'to', 'play', 'therein', '.\"', '--', 'psalms', '.', '\"', 'in', 'that', 'day', ',', 'the', 'lord', 'with', 'his', 'sore', ',', 'and', 'great', ',', 'and', 'strong', 'sword', ',', 'shall', 'punish', 'leviathan', 'the', 'piercing', 'serpent', ',', 'even', 'leviathan', 'that', 'crooked', 'serpent', ';', 'and', 'he', 'shall', 'slay', 'the', 'dragon', 'that', 'is', 'in', 'the', 'sea', '.\"', '--', 'isaiah', '\"', 'and', 'what', 'thing', 'soever', 'besides', 'cometh', 'within', 'the', 'chaos', 'of', 'this', 'monster', \"'\", 's', 'mouth', ',', 'be', 'it', 'beast', ',', 'boat', ',', 'or', 'stone', ',', 'down', 'it', 'goes', 'all', 'incontinently', 'that', 'foul', 'great', 'swallow', 'of', 'his', ',', 'and', 'perisheth', 'in', 'the', 'bottomless', 'gulf', 'of', 'his', 'paunch', '.\"', '--', 'holland', \"'\", 's', 'plutarch', \"'\", 's', 'morals', '.', '\"', 'the', 'indian', 'sea', 'breedeth', 'the', 'most', 'and', 'the', 'biggest', 'fishes', 'that', 'are', ':', 'among', 'which', 'the', 'whales', 'and', 'whirlpooles', 'called', 'balaene', ',', 'take', 'up', 'as', 'much', 'in', 'length', 'as', 'four', 'acres', 'or', 'arpens', 'of', 'land', '.\"', '--', 'holland', \"'\", 's', 'pliny', '.', '\"', 'scarcely', 'had', 'we', 'proceeded', 'two', 'days', 'on', 'the', 'sea', ',', 'when', 'about', 'sunrise', 'a', 'great', 'many', 'whales', 'and', 'other', 'monsters', 'of', 'the', 'sea', ',', 'appeared', '.', 'among', 'the', 'former', ',', 'one', 'was', 'of', 'a', 'most', 'monstrous', 'size', '.', '...', 'this', 'came', 'towards', 'us', ',', 'open', '-', 'mouthed', ',', 'raising', 'the', 'waves', 'on', 'all', 'sides', ',', 'and', 'beating', 'the', 'sea', 'before', 'him', 'into', 'a', 'foam', '.\"', '--', 'tooke', \"'\", 's', 'lucian', '.', '\"', 'the', 'true', 'history', '.\"', '\"', 'he', 'visited', 'this', 'country', 'also', 'with', 'a', 'view', 'of', 'catching', 'horse', '-', 'whales', ',', 'which', 'had', 'bones', 'of', 'very', 'great', 'value', 'for', 'their', 'teeth', ',', 'of', 'which', 'he', 'brought', 'some', 'to', 'the', 'king', '.', '...', 'the', 'best', 'whales', 'were', 'catched', 'in', 'his', 'own', 'country', ',', 'of', 'which', 'some', 'were', 'forty', '-', 'eight', ',', 'some', 'fifty', 'yards', 'long', '.', 'he', 'said', 'that', 'he', 'was', 'one', 'of', 'six', 'who', 'had', 'killed', 'sixty', 'in', 'two', 'days', '.\"', '--', 'other', 'or', 'octher', \"'\", 's', 'verbal', 'narrative', 'taken', 'down', 'from', 'his', 'mouth', 'by', 'king', 'alfred', ',', 'a', '.', 'd', '.', '890', '.', '\"', 'and', 'whereas', 'all', 'the', 'other', 'things', ',', 'whether', 'beast', 'or', 'vessel', ',', 'that', 'enter', 'into', 'the', 'dreadful', 'gulf', 'of', 'this', 'monster', \"'\", 's', '(', 'whale', \"'\", 's', ')', 'mouth', ',', 'are', 'immediately', 'lost', 'and', 'swallowed', 'up', ',', 'the', 'sea', '-', 'gudgeon', 'retires', 'into', 'it', 'in', 'great', 'security', ',', 'and', 'there', 'sleeps', '.\"', '--', 'montaigne', '.', '--', 'apology', 'for', 'raimond', 'sebond', '.', '\"', 'let', 'us', 'fly', ',', 'let', 'us', 'fly', '!', 'old', 'nick', 'take', 'me', 'if', 'is', 'not', 'leviathan', 'described', 'by', 'the', 'noble', 'prophet', 'moses', 'in', 'the', 'life', 'of', 'patient', 'job', '.\"', '--', 'rabelais', '.', '\"', 'this', 'whale', \"'\", 's', 'liver', 'was', 'two', 'cartloads', '.\"', '--', 'stowe', \"'\", 's', 'annals', '.', '\"', 'the', 'great', 'leviathan', 'that', 'maketh', 'the', 'seas', 'to', 'seethe', 'like', 'boiling', 'pan', '.\"', '--', 'lord', 'bacon', \"'\", 's', 'version', 'of', 'the', 'psalms', '.', '\"', 'touching', 'that', 'monstrous', 'bulk', 'of', 'the', 'whale', 'or', 'ork', 'we', 'have', 'received', 'nothing', 'certain', '.', 'they', 'grow', 'exceeding', 'fat', ',', 'insomuch', 'that', 'an', 'incredible', 'quantity', 'of', 'oil', 'will', 'be', 'extracted', 'out', 'of', 'one', 'whale', '.\"', '--', 'ibid', '.', '\"', 'history', 'of', 'life', 'and', 'death', '.\"', '\"', 'the', 'sovereignest', 'thing', 'on', 'earth', 'is', 'parmacetti', 'for', 'an', 'inward', 'bruise', '.\"', '--', 'king', 'henry', '.', '\"', 'very', 'like', 'a', 'whale', '.\"', '--', 'hamlet', '.', '\"', 'which', 'to', 'secure', ',', 'no', 'skill', 'of', 'leach', \"'\", 's', 'art', 'mote', 'him', 'availle', ',', 'but', 'to', 'returne', 'againe', 'to', 'his', 'wound', \"'\", 's', 'worker', ',', 'that', 'with', 'lowly', 'dart', ',', 'dinting', 'his', 'breast', ',', 'had', 'bred', 'his', 'restless', 'paine', ',', 'like', 'as', 'the', 'wounded', 'whale', 'to', 'shore', 'flies', 'thro', \"'\", 'the', 'maine', '.\"', '--', 'the', 'faerie', 'queen', '.', '\"', 'immense', 'as', 'whales', ',', 'the', 'motion', 'of', 'whose', 'vast', 'bodies', 'can', 'in', 'a', 'peaceful', 'calm', 'trouble', 'the', 'ocean', 'til', 'it', 'boil', '.\"', '--', 'sir', 'william', 'davenant', '.', 'preface', 'to', 'gondibert', '.', '\"', 'what', 'spermacetti', 'is', ',', 'men', 'might', 'justly', 'doubt', ',', 'since', 'the', 'learned', 'hosmannus', 'in', 'his', 'work', 'of', 'thirty', 'years', ',', 'saith', 'plainly', ',', 'nescio', 'quid', 'sit', '.\"', '--', 'sir', 't', '.', 'browne', '.', 'of', 'sperma', 'ceti', 'and', 'the', 'sperma', 'ceti', 'whale', '.', 'vide', 'his', 'v', '.', 'e', '.', '\"', 'like', 'spencer', \"'\", 's', 'talus', 'with', 'his', 'modern', 'flail', 'he', 'threatens', 'ruin', 'with', 'his', 'ponderous', 'tail', '.', '...', 'their', 'fixed', 'jav', \"'\", 'lins', 'in', 'his', 'side', 'he', 'wears', ',', 'and', 'on', 'his', 'back', 'a', 'grove', 'of', 'pikes', 'appears', '.\"', '--', 'waller', \"'\", 's', 'battle', 'of', 'the', 'summer', 'islands', '.', '\"', 'by', 'art', 'is', 'created', 'that', 'great', 'leviathan', ',', 'called', 'a', 'commonwealth', 'or', 'state', '--(', 'in', 'latin', ',', 'civitas', ')', 'which', 'is', 'but', 'an', 'artificial', 'man', '.\"']\n"
     ]
    }
   ],
   "source": [
    "print(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "border = int(len(word_count) * 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 96, '.': 66, 'the': 58, 'of': 36, 'and': 35, '--': 27, '\"': 26, '.\"': 26, 'to': 25, '-': 24, 'a': 21, 'in': 20, \"'\": 20, 's': 19, 'his': 17, 'that': 17, 'whale': 13, ';': 12, 'is': 12, 'by': 10, 'he': 10, 'with': 10, 'or': 10, 'for': 10, 'it': 9, 'which': 9, 'great': 9, 'this': 8, 'as': 8, 'leviathan': 8, 'sub': 7, 'whales': 7, 'be': 6, 'sea': 6, 'him': 5, 'all': 5, 'take': 5, '...': 5, 'up': 5, 'but': 5, 'one': 5, '!': 5, 'ye': 5, 'had': 5, 'was': 4, 'what': 4, 'out': 4, 'not': 4, 'from': 4, 'nuee': 4, 'extracts': 4, 'are': 4, 'shall': 4, 'your': 4, 'there': 4, 'on': 4, 'like': 4, '(': 3, ')': 3, 'i': 3, 'ever': 3, 'old': 3, 'world': 3, 'you': 3, 'called': 3, 'maketh': 3, 'more': 3, 'will': 3, 'poor': 3, 'have': 3, 'long': 3, 'whom': 3, 'would': 3, 'much': 3, 'down': 3, 'lord': 3, 'mouth': 3, 'two': 3, 'other': 3, 'us': 3, 'into': 3, 'some': 3, 'king': 3, 'an': 3, 'supplied': 2, 'usher': 2, 'school': 2, 'pale': 2, 'now': 2, 'grammars': 2, 'nations': 2, 'them': 2, 'fish': 2, 'our': 2, 'through': 2, 'true': 2, 'dan': 2, 'hvalt': 2, 'dictionary': 2, 'immediately': 2, 'latin': 2, 'pekee': 2, 'devil': 2, 'appears': 2, 'gone': 2, 'earth': 2, 'could': 2, 'these': 2, 'touching': 2, 'well': 2, 'here': 2, 'view': 2, 'said': 2, 'many': 2, 'own': 2, 'so': 2, 'whose': 2, 'thou': 2, 'no': 2, 'even': 2, 'too': 2, 'strong': 2, 'sit': 2, 'grow': 2, 'tears': 2, 'glasses': 2, 'go': 2, 'hearts': 2, 'who': 2, 'before': 2, 'strike': 2, 'created': 2, 'job': 2, 'swallow': 2, 'jonah': 2, 'psalms': 2, 'serpent': 2, 'thing': 2, 'monster': 2, 'beast': 2, 'gulf': 2, 'holland': 2, 'most': 2, 'among': 2, 'we': 2, 'days': 2, 'monstrous': 2, 'history': 2, 'country': 2, 'very': 2, 'their': 2, 'were': 2, 'let': 2, 'fly': 2, 'life': 2, 'art': 2, 'sir': 2, 'sperma': 2, 'ceti': 2, '[': 1, 'moby': 1, 'dick': 1, 'herman': 1, 'melville': 1, '1851': 1, ']': 1, 'etymology': 1, 'late': 1, 'consumptive': 1, 'grammar': 1, 'threadbare': 1, 'coat': 1, 'heart': 1, 'body': 1, 'brain': 1, 'see': 1, 'dusting': 1, 'lexicons': 1, 'queer': 1, 'handkerchief': 1, 'mockingly': 1, 'embellished': 1, 'gay': 1, 'flags': 1, 'known': 1, 'loved': 1, 'dust': 1, 'somehow': 1, 'mildly': 1, 'reminded': 1, 'mortality': 1, 'while': 1, 'hand': 1, 'others': 1, 'teach': 1, 'name': 1, 'tongue': 1, 'leaving': 1, 'ignorance': 1, 'letter': 1, 'h': 1, 'almost': 1, 'alone': 1, 'signification': 1, 'word': 1, 'deliver': 1, 'hackluyt': 1, 'sw': 1, 'hval': 1, 'animal': 1, 'named': 1, 'roundness': 1, 'rolling': 1, 'arched': 1, 'vaulted': 1, 'webster': 1, 'dut': 1, 'ger': 1, 'wallen': 1, 'walw': 1, 'ian': 1, 'roll': 1, 'wallow': 1, 'richardson': 1, 'ketos': 1, 'greek': 1, 'cetus': 1, 'whoel': 1, 'anglo': 1, 'saxon': 1, 'danish': 1, 'wal': 1, 'dutch': 1, 'hwal': 1, 'swedish': 1, 'icelandic': 1, 'english': 1, 'baleine': 1, 'french': 1, 'ballena': 1, 'spanish': 1, 'fegee': 1, 'erromangoan': 1, 'librarian': 1, ').': 1, 'seen': 1, 'mere': 1, 'painstaking': 1, 'burrower': 1, 'grub': 1, 'worm': 1, 'vaticans': 1, 'street': 1, 'stalls': 1, 'picking': 1, 'whatever': 1, 'random': 1, 'allusions': 1, 'anyways': 1, 'find': 1, 'any': 1, 'book': 1, 'whatsoever': 1, 'sacred': 1, 'profane': 1, 'therefore': 1, 'must': 1, 'every': 1, 'case': 1, 'at': 1, 'least': 1, 'higgledy': 1, 'piggledy': 1, 'statements': 1, 'however': 1, 'authentic': 1, 'veritable': 1, 'gospel': 1, 'cetology': 1, 'far': 1, 'ancient': 1, 'authors': 1, 'generally': 1, 'poets': 1, 'appearing': 1, 'solely': 1, 'valuable': 1, 'entertaining': 1, 'affording': 1, 'glancing': 1, 'bird': 1, 'eye': 1, 'has': 1, 'been': 1, 'promiscuously': 1, 'thought': 1, 'fancied': 1, 'sung': 1, 'generations': 1, 'including': 1, 'fare': 1, 'thee': 1, 'commentator': 1, 'am': 1, 'belongest': 1, 'hopeless': 1, 'sallow': 1, 'tribe': 1, 'wine': 1, 'warm': 1, 'sherry': 1, 'rosy': 1, 'sometimes': 1, 'loves': 1, 'feel': 1, 'devilish': 1, 'convivial': 1, 'upon': 1, 'say': 1, 'bluntly': 1, 'full': 1, 'eyes': 1, 'empty': 1, 'altogether': 1, 'unpleasant': 1, 'sadness': 1, 'give': 1, 'subs': 1, 'how': 1, 'pains': 1, 'please': 1, 'thankless': 1, 'clear': 1, 'hampton': 1, 'court': 1, 'tuileries': 1, 'gulp': 1, 'hie': 1, 'aloft': 1, 'royal': 1, 'mast': 1, 'friends': 1, 'clearing': 1, 'seven': 1, 'storied': 1, 'heavens': 1, 'making': 1, 'refugees': 1, 'pampered': 1, 'gabriel': 1, 'michael': 1, 'raphael': 1, 'against': 1, 'coming': 1, 'splintered': 1, 'together': 1, 'unsplinterable': 1, 'god': 1, 'genesis': 1, 'path': 1, 'shine': 1, 'after': 1, 'think': 1, 'deep': 1, 'hoary': 1, 'prepared': 1, 'ships': 1, 'hast': 1, 'made': 1, 'play': 1, 'therein': 1, 'day': 1, 'sore': 1, 'sword': 1, 'punish': 1, 'piercing': 1, 'crooked': 1, 'slay': 1, 'dragon': 1, 'isaiah': 1, 'soever': 1, 'besides': 1, 'cometh': 1, 'within': 1, 'chaos': 1, 'boat': 1, 'stone': 1, 'goes': 1, 'incontinently': 1, 'foul': 1, 'perisheth': 1, 'bottomless': 1, 'paunch': 1, 'plutarch': 1, 'morals': 1, 'indian': 1, 'breedeth': 1, 'biggest': 1, 'fishes': 1, ':': 1, 'whirlpooles': 1, 'balaene': 1, 'length': 1, 'four': 1, 'acres': 1, 'arpens': 1, 'land': 1, 'pliny': 1, 'scarcely': 1, 'proceeded': 1, 'when': 1, 'about': 1, 'sunrise': 1, 'monsters': 1, 'appeared': 1, 'former': 1, 'size': 1, 'came': 1, 'towards': 1, 'open': 1, 'mouthed': 1, 'raising': 1, 'waves': 1, 'sides': 1, 'beating': 1, 'foam': 1, 'tooke': 1, 'lucian': 1, 'visited': 1, 'also': 1, 'catching': 1, 'horse': 1, 'bones': 1, 'value': 1, 'teeth': 1, 'brought': 1, 'best': 1, 'catched': 1, 'forty': 1, 'eight': 1, 'fifty': 1, 'yards': 1, 'six': 1, 'killed': 1, 'sixty': 1, 'octher': 1, 'verbal': 1, 'narrative': 1, 'taken': 1, 'alfred': 1, 'd': 1, '890': 1, 'whereas': 1, 'things': 1, 'whether': 1, 'vessel': 1, 'enter': 1, 'dreadful': 1, 'lost': 1, 'swallowed': 1, 'gudgeon': 1, 'retires': 1, 'security': 1, 'sleeps': 1, 'montaigne': 1, 'apology': 1, 'raimond': 1, 'sebond': 1, 'nick': 1, 'me': 1, 'if': 1, 'described': 1, 'noble': 1, 'prophet': 1, 'moses': 1, 'patient': 1, 'rabelais': 1, 'liver': 1, 'cartloads': 1, 'stowe': 1, 'annals': 1, 'seas': 1, 'seethe': 1, 'boiling': 1, 'pan': 1, 'bacon': 1, 'version': 1, 'bulk': 1, 'ork': 1, 'received': 1, 'nothing': 1, 'certain': 1, 'they': 1, 'exceeding': 1, 'fat': 1, 'insomuch': 1, 'incredible': 1, 'quantity': 1, 'oil': 1, 'extracted': 1, 'ibid': 1, 'death': 1, 'sovereignest': 1, 'parmacetti': 1, 'inward': 1, 'bruise': 1, 'henry': 1, 'hamlet': 1, 'secure': 1, 'skill': 1, 'leach': 1, 'mote': 1, 'availle': 1, 'returne': 1, 'againe': 1, 'wound': 1, 'worker': 1, 'lowly': 1, 'dart': 1, 'dinting': 1, 'breast': 1, 'bred': 1, 'restless': 1, 'paine': 1, 'wounded': 1, 'shore': 1, 'flies': 1, 'thro': 1, 'maine': 1, 'faerie': 1, 'queen': 1, 'immense': 1, 'motion': 1, 'vast': 1, 'bodies': 1, 'can': 1, 'peaceful': 1, 'calm': 1, 'trouble': 1, 'ocean': 1, 'til': 1, 'boil': 1, 'william': 1, 'davenant': 1, 'preface': 1, 'gondibert': 1, 'spermacetti': 1, 'men': 1, 'might': 1, 'justly': 1, 'doubt': 1, 'since': 1, 'learned': 1, 'hosmannus': 1, 'work': 1, 'thirty': 1, 'years': 1, 'saith': 1, 'plainly': 1, 'nescio': 1, 'quid': 1, 't': 1, 'browne': 1, 'vide': 1, 'v': 1, 'e': 1, 'spencer': 1, 'talus': 1, 'modern': 1, 'flail': 1, 'threatens': 1, 'ruin': 1, 'ponderous': 1, 'tail': 1, 'fixed': 1, 'jav': 1, 'lins': 1, 'side': 1, 'wears': 1, 'back': 1, 'grove': 1, 'pikes': 1, 'waller': 1, 'battle': 1, 'summer': 1, 'islands': 1, 'commonwealth': 1, 'state': 1, '--(': 1, 'civitas': 1, 'artificial': 1, 'man': 1})\n"
     ]
    }
   ],
   "source": [
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n"
     ]
    }
   ],
   "source": [
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 5 most common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 96), ('.', 66), ('the', 58), ('of', 36), ('and', 35)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revstopwords= list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 1), ('artificial', 1), ('civitas', 1), ('--(', 1), ('state', 1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revstopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Stopwords from unigram distribution's tails || Most used and least used are stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = word_count.most_common()[:border] + list(reversed(word_count.most_common()))[:border]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 96),\n",
       " ('.', 66),\n",
       " ('the', 58),\n",
       " ('of', 36),\n",
       " ('and', 35),\n",
       " ('man', 1),\n",
       " ('artificial', 1),\n",
       " ('civitas', 1),\n",
       " ('--(', 1),\n",
       " ('state', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [s[0] for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(stopwords))\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bruise',\n",
       " 'baleine',\n",
       " 'school',\n",
       " 'ever',\n",
       " 'through',\n",
       " 'veritable',\n",
       " 'mockingly',\n",
       " 'hvalt',\n",
       " 'brain',\n",
       " 'six',\n",
       " 'art',\n",
       " 'erromangoan',\n",
       " 'which',\n",
       " 'immediately',\n",
       " 'catched',\n",
       " 'prepared',\n",
       " 'alone',\n",
       " 'ancient',\n",
       " 'they',\n",
       " 'ceti']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 583\n"
     ]
    }
   ],
   "source": [
    "print(len(set(flatten(corpus))), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {'<UNK>' : 0} \n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](training_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by'),\n",
       " ('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman'),\n",
       " ('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville'),\n",
       " ('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a n-grams model\n",
    "Training data will be :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by')]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "\n",
    "print(train_data[:WINDOW_SIZE * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[', 'moby')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[413]]),\n",
       " tensor([[413]]),\n",
       " tensor([[413]]),\n",
       " tensor([[556]]),\n",
       " tensor([[556]]),\n",
       " tensor([[556]]),\n",
       " tensor([[556]]),\n",
       " tensor([[576]]),\n",
       " tensor([[576]]),\n",
       " tensor([[576]]),\n",
       " tensor([[576]]),\n",
       " tensor([[576]]),\n",
       " tensor([[290]]),\n",
       " tensor([[290]]),\n",
       " tensor([[290]]),\n",
       " tensor([[290]]),\n",
       " tensor([[290]]),\n",
       " tensor([[290]]),\n",
       " tensor([[50]]),\n",
       " tensor([[50]])]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_p[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[556]]),\n",
       " tensor([[576]]),\n",
       " tensor([[290]]),\n",
       " tensor([[413]]),\n",
       " tensor([[576]]),\n",
       " tensor([[290]]),\n",
       " tensor([[50]]),\n",
       " tensor([[413]]),\n",
       " tensor([[556]]),\n",
       " tensor([[290]]),\n",
       " tensor([[50]]),\n",
       " tensor([[501]]),\n",
       " tensor([[413]]),\n",
       " tensor([[556]]),\n",
       " tensor([[576]]),\n",
       " tensor([[50]]),\n",
       " tensor([[501]]),\n",
       " tensor([[58]]),\n",
       " tensor([[556]]),\n",
       " tensor([[576]])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # init\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # init\n",
    "        #self.out = nn.Linear(projection_dim,vocab_size)\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        outer_embeds = self.embedding_u(outer_words) # B x V x D\n",
    "        \n",
    "        # batchwise matrix multiplication\n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1xD * BxDx1 => Bx1\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # BxVxD * BxDx1 => BxV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 6.17\n",
      "Epoch : 10, mean_loss : 4.37\n",
      "Epoch : 20, mean_loss : 3.48\n",
      "Epoch : 30, mean_loss : 3.31\n",
      "Epoch : 40, mean_loss : 3.26\n",
      "Epoch : 50, mean_loss : 3.24\n",
      "Epoch : 60, mean_loss : 3.23\n",
      "Epoch : 70, mean_loss : 3.21\n",
      "Epoch : 80, mean_loss : 3.21\n",
      "Epoch : 90, mean_loss : 3.20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) # B x 1\n",
    "        targets = torch.cat(targets) # B x 1\n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))  # B x V\n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "        #negative log likelihood\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "        losses.append(loss.data.tolist())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        ...,\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0],\n",
       "        [  1,   2,   3,  ..., 581, 582,   0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: continue\n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] \n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10] # sort by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['book', 0.7919671535491943],\n",
       " ['find', 0.660162091255188],\n",
       " ['picking', 0.5818719863891602],\n",
       " ['whatsoever', 0.5688313841819763],\n",
       " ['anyways', 0.5544962882995605],\n",
       " ['vast', 0.5483270287513733],\n",
       " ['nothing', 0.5259244441986084],\n",
       " ['gulf', 0.5102213621139526],\n",
       " ['nescio', 0.5036137700080872],\n",
       " ['own', 0.4889150857925415]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
